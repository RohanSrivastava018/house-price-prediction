{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "import pickle\n",
    "import kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
      "0  13300000  7420         4          2        3      yes        no       no   \n",
      "1  12250000  8960         4          4        4      yes        no       no   \n",
      "2  12250000  9960         3          2        2      yes        no      yes   \n",
      "3  12215000  7500         4          2        2      yes        no      yes   \n",
      "4  11410000  7420         4          1        2      yes       yes      yes   \n",
      "\n",
      "  hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
      "0              no             yes        2      yes        furnished  \n",
      "1              no             yes        3       no        furnished  \n",
      "2              no              no        2      yes   semi-furnished  \n",
      "3              no             yes        3      yes        furnished  \n",
      "4              no             yes        2       no        furnished  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 545 entries, 0 to 544\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   price             545 non-null    int64 \n",
      " 1   area              545 non-null    int64 \n",
      " 2   bedrooms          545 non-null    int64 \n",
      " 3   bathrooms         545 non-null    int64 \n",
      " 4   stories           545 non-null    int64 \n",
      " 5   mainroad          545 non-null    object\n",
      " 6   guestroom         545 non-null    object\n",
      " 7   basement          545 non-null    object\n",
      " 8   hotwaterheating   545 non-null    object\n",
      " 9   airconditioning   545 non-null    object\n",
      " 10  parking           545 non-null    int64 \n",
      " 11  prefarea          545 non-null    object\n",
      " 12  furnishingstatus  545 non-null    object\n",
      "dtypes: int64(6), object(7)\n",
      "memory usage: 55.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Loading in dataset using kaggle api\n",
    "kaggle.api.dataset_download_file('harishkumardatalab/housing-price-prediction', 'housing.csv')\n",
    "\n",
    "df = pd.read_csv('housing.csv')\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(545, 13)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features and labels\n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohansrivastava/miniconda3/envs/ml_env/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Encoding the binary string inputs (yer or no)\n",
    "binary_categorical_features = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
    "label_encoder = LabelEncoder()\n",
    "for feature in binary_categorical_features:\n",
    "    X[feature] = label_encoder.fit_transform(X[feature])\n",
    "\n",
    "# Encoding the furnishingstatus feature (furnished, semi-furnished, or unfurnished)\n",
    "three_categorical_feature = 'furnishingstatus'\n",
    "onehot_encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "furnishingstatus_encoded = onehot_encoder.fit_transform(X[[three_categorical_feature]])\n",
    "furnishingstatus_encoded_df = pd.DataFrame(furnishingstatus_encoded, columns=onehot_encoder.get_feature_names_out([three_categorical_feature]))\n",
    "X = pd.concat([X.drop(three_categorical_feature, axis=1), furnishingstatus_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splittin data into train, val, and test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1))  # Single output neuron for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_18 (Dense)            (None, 64)                960       \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3073 (12.00 KB)\n",
      "Trainable params: 3073 (12.00 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compiling model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 25822064803840.0000 - val_loss: 27060953153536.0000\n",
      "Epoch 2/1500\n",
      "48/48 [==============================] - 0s 803us/step - loss: 25822024957952.0000 - val_loss: 27060890238976.0000\n",
      "Epoch 3/1500\n",
      "48/48 [==============================] - 0s 758us/step - loss: 25821926391808.0000 - val_loss: 27060745535488.0000\n",
      "Epoch 4/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 25821712482304.0000 - val_loss: 27060451934208.0000\n",
      "Epoch 5/1500\n",
      "48/48 [==============================] - 0s 697us/step - loss: 25821343383552.0000 - val_loss: 27059925549056.0000\n",
      "Epoch 6/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 25820720529408.0000 - val_loss: 27059095076864.0000\n",
      "Epoch 7/1500\n",
      "48/48 [==============================] - 0s 711us/step - loss: 25819768422400.0000 - val_loss: 27057849368576.0000\n",
      "Epoch 8/1500\n",
      "48/48 [==============================] - 0s 702us/step - loss: 25818287833088.0000 - val_loss: 27056098246656.0000\n",
      "Epoch 9/1500\n",
      "48/48 [==============================] - 0s 687us/step - loss: 25816444436480.0000 - val_loss: 27053738950656.0000\n",
      "Epoch 10/1500\n",
      "48/48 [==============================] - 0s 692us/step - loss: 25813858648064.0000 - val_loss: 27050626777088.0000\n",
      "Epoch 11/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 25810876497920.0000 - val_loss: 27046948372480.0000\n",
      "Epoch 12/1500\n",
      "48/48 [==============================] - 0s 688us/step - loss: 25806621376512.0000 - val_loss: 27042252849152.0000\n",
      "Epoch 13/1500\n",
      "48/48 [==============================] - 0s 703us/step - loss: 25802313826304.0000 - val_loss: 27036754116608.0000\n",
      "Epoch 14/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 25796565532672.0000 - val_loss: 27030198419456.0000\n",
      "Epoch 15/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 25790280368128.0000 - val_loss: 27022537523200.0000\n",
      "Epoch 16/1500\n",
      "48/48 [==============================] - 0s 691us/step - loss: 25783391223808.0000 - val_loss: 27013878382592.0000\n",
      "Epoch 17/1500\n",
      "48/48 [==============================] - 0s 707us/step - loss: 25773683507200.0000 - val_loss: 27003820441600.0000\n",
      "Epoch 18/1500\n",
      "48/48 [==============================] - 0s 702us/step - loss: 25763948527616.0000 - val_loss: 26992831365120.0000\n",
      "Epoch 19/1500\n",
      "48/48 [==============================] - 0s 698us/step - loss: 25753475350528.0000 - val_loss: 26979862577152.0000\n",
      "Epoch 20/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 25741204914176.0000 - val_loss: 26966117842944.0000\n",
      "Epoch 21/1500\n",
      "48/48 [==============================] - 0s 677us/step - loss: 25728231931904.0000 - val_loss: 26950938656768.0000\n",
      "Epoch 22/1500\n",
      "48/48 [==============================] - 0s 705us/step - loss: 25714789187584.0000 - val_loss: 26934826237952.0000\n",
      "Epoch 23/1500\n",
      "48/48 [==============================] - 0s 982us/step - loss: 25700807475200.0000 - val_loss: 26916480352256.0000\n",
      "Epoch 24/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 25678758019072.0000 - val_loss: 26897444503552.0000\n",
      "Epoch 25/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 25660137406464.0000 - val_loss: 26876466692096.0000\n",
      "Epoch 26/1500\n",
      "48/48 [==============================] - 0s 706us/step - loss: 25640977825792.0000 - val_loss: 26853590958080.0000\n",
      "Epoch 27/1500\n",
      "48/48 [==============================] - 0s 707us/step - loss: 25620473970688.0000 - val_loss: 26829245120512.0000\n",
      "Epoch 28/1500\n",
      "48/48 [==============================] - 0s 696us/step - loss: 25597797466112.0000 - val_loss: 26802894405632.0000\n",
      "Epoch 29/1500\n",
      "48/48 [==============================] - 0s 691us/step - loss: 25577855647744.0000 - val_loss: 26776187174912.0000\n",
      "Epoch 30/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 25555948797952.0000 - val_loss: 26748162932736.0000\n",
      "Epoch 31/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 25520439820288.0000 - val_loss: 26717003448320.0000\n",
      "Epoch 32/1500\n",
      "48/48 [==============================] - 0s 696us/step - loss: 25490656067584.0000 - val_loss: 26684143173632.0000\n",
      "Epoch 33/1500\n",
      "48/48 [==============================] - 0s 688us/step - loss: 25457942593536.0000 - val_loss: 26650238517248.0000\n",
      "Epoch 34/1500\n",
      "48/48 [==============================] - 0s 681us/step - loss: 25434026672128.0000 - val_loss: 26615211884544.0000\n",
      "Epoch 35/1500\n",
      "48/48 [==============================] - 0s 688us/step - loss: 25403729117184.0000 - val_loss: 26578601902080.0000\n",
      "Epoch 36/1500\n",
      "48/48 [==============================] - 0s 689us/step - loss: 25363579142144.0000 - val_loss: 26538575659008.0000\n",
      "Epoch 37/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 25323538219008.0000 - val_loss: 26497429536768.0000\n",
      "Epoch 38/1500\n",
      "48/48 [==============================] - 0s 694us/step - loss: 25291447599104.0000 - val_loss: 26452781170688.0000\n",
      "Epoch 39/1500\n",
      "48/48 [==============================] - 0s 683us/step - loss: 25238783918080.0000 - val_loss: 26409355444224.0000\n",
      "Epoch 40/1500\n",
      "48/48 [==============================] - 0s 665us/step - loss: 25190448758784.0000 - val_loss: 26361441812480.0000\n",
      "Epoch 41/1500\n",
      "48/48 [==============================] - 0s 673us/step - loss: 25145167052800.0000 - val_loss: 26311357628416.0000\n",
      "Epoch 42/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 25116666757120.0000 - val_loss: 26261657223168.0000\n",
      "Epoch 43/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 25063013220352.0000 - val_loss: 26208936919040.0000\n",
      "Epoch 44/1500\n",
      "48/48 [==============================] - 0s 704us/step - loss: 25019266629632.0000 - val_loss: 26155587469312.0000\n",
      "Epoch 45/1500\n",
      "48/48 [==============================] - 0s 676us/step - loss: 24978969853952.0000 - val_loss: 26099775963136.0000\n",
      "Epoch 46/1500\n",
      "48/48 [==============================] - 0s 676us/step - loss: 24895884886016.0000 - val_loss: 26040896323584.0000\n",
      "Epoch 47/1500\n",
      "48/48 [==============================] - 0s 682us/step - loss: 24852178141184.0000 - val_loss: 25981133783040.0000\n",
      "Epoch 48/1500\n",
      "48/48 [==============================] - 0s 698us/step - loss: 24806468616192.0000 - val_loss: 25919110512640.0000\n",
      "Epoch 49/1500\n",
      "48/48 [==============================] - 0s 707us/step - loss: 24740733386752.0000 - val_loss: 25854597922816.0000\n",
      "Epoch 50/1500\n",
      "48/48 [==============================] - 0s 680us/step - loss: 24673725186048.0000 - val_loss: 25790290853888.0000\n",
      "Epoch 51/1500\n",
      "48/48 [==============================] - 0s 687us/step - loss: 24606549213184.0000 - val_loss: 25721753829376.0000\n",
      "Epoch 52/1500\n",
      "48/48 [==============================] - 0s 677us/step - loss: 24578009071616.0000 - val_loss: 25652256309248.0000\n",
      "Epoch 53/1500\n",
      "48/48 [==============================] - 0s 688us/step - loss: 24459681464320.0000 - val_loss: 25580590333952.0000\n",
      "Epoch 54/1500\n",
      "48/48 [==============================] - 0s 675us/step - loss: 24423396540416.0000 - val_loss: 25507007561728.0000\n",
      "Epoch 55/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 24369925455872.0000 - val_loss: 25431898062848.0000\n",
      "Epoch 56/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 24314959101952.0000 - val_loss: 25355849039872.0000\n",
      "Epoch 57/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 24262368821248.0000 - val_loss: 25278868881408.0000\n",
      "Epoch 58/1500\n",
      "48/48 [==============================] - 0s 758us/step - loss: 24138422943744.0000 - val_loss: 25200559128576.0000\n",
      "Epoch 59/1500\n",
      "48/48 [==============================] - 0s 786us/step - loss: 24039290568704.0000 - val_loss: 25116100526080.0000\n",
      "Epoch 60/1500\n",
      "48/48 [==============================] - 0s 772us/step - loss: 24013126500352.0000 - val_loss: 25034621976576.0000\n",
      "Epoch 61/1500\n",
      "48/48 [==============================] - 0s 764us/step - loss: 23909409751040.0000 - val_loss: 24946818416640.0000\n",
      "Epoch 62/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 23852920864768.0000 - val_loss: 24861009248256.0000\n",
      "Epoch 63/1500\n",
      "48/48 [==============================] - 0s 807us/step - loss: 23739060191232.0000 - val_loss: 24773744656384.0000\n",
      "Epoch 64/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 23642721222656.0000 - val_loss: 24680968749056.0000\n",
      "Epoch 65/1500\n",
      "48/48 [==============================] - 0s 769us/step - loss: 23600960634880.0000 - val_loss: 24589908312064.0000\n",
      "Epoch 66/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 23531609915392.0000 - val_loss: 24494909423616.0000\n",
      "Epoch 67/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 23493244616704.0000 - val_loss: 24400741007360.0000\n",
      "Epoch 68/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 23383303520256.0000 - val_loss: 24303986802688.0000\n",
      "Epoch 69/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 23328731430912.0000 - val_loss: 24214803316736.0000\n",
      "Epoch 70/1500\n",
      "48/48 [==============================] - 0s 784us/step - loss: 23146352607232.0000 - val_loss: 24110866366464.0000\n",
      "Epoch 71/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 23170713124864.0000 - val_loss: 24011281006592.0000\n",
      "Epoch 72/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 23008261439488.0000 - val_loss: 23906014461952.0000\n",
      "Epoch 73/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 22894694367232.0000 - val_loss: 23801941196800.0000\n",
      "Epoch 74/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 22779124514816.0000 - val_loss: 23696622223360.0000\n",
      "Epoch 75/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 22635924684800.0000 - val_loss: 23585749991424.0000\n",
      "Epoch 76/1500\n",
      "48/48 [==============================] - 0s 930us/step - loss: 22603794219008.0000 - val_loss: 23477021048832.0000\n",
      "Epoch 77/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 22570401267712.0000 - val_loss: 23367331610624.0000\n",
      "Epoch 78/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 22451199148032.0000 - val_loss: 23253852618752.0000\n",
      "Epoch 79/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 22263457906688.0000 - val_loss: 23140931469312.0000\n",
      "Epoch 80/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 22224813686784.0000 - val_loss: 23021865664512.0000\n",
      "Epoch 81/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 22088756756480.0000 - val_loss: 22901669494784.0000\n",
      "Epoch 82/1500\n",
      "48/48 [==============================] - 0s 706us/step - loss: 21956179001344.0000 - val_loss: 22785648754688.0000\n",
      "Epoch 83/1500\n",
      "48/48 [==============================] - 0s 693us/step - loss: 21905258053632.0000 - val_loss: 22667740577792.0000\n",
      "Epoch 84/1500\n",
      "48/48 [==============================] - 0s 730us/step - loss: 21686980182016.0000 - val_loss: 22541473153024.0000\n",
      "Epoch 85/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 21607726710784.0000 - val_loss: 22416168321024.0000\n",
      "Epoch 86/1500\n",
      "48/48 [==============================] - 0s 787us/step - loss: 21481381691392.0000 - val_loss: 22289804427264.0000\n",
      "Epoch 87/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 21415235420160.0000 - val_loss: 22163962724352.0000\n",
      "Epoch 88/1500\n",
      "48/48 [==============================] - 0s 748us/step - loss: 21385120317440.0000 - val_loss: 22039689691136.0000\n",
      "Epoch 89/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 21124503044096.0000 - val_loss: 21909875982336.0000\n",
      "Epoch 90/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 20925986635776.0000 - val_loss: 21777031888896.0000\n",
      "Epoch 91/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 20904430010368.0000 - val_loss: 21642440867840.0000\n",
      "Epoch 92/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 20795529101312.0000 - val_loss: 21509554831360.0000\n",
      "Epoch 93/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 20703281676288.0000 - val_loss: 21377050476544.0000\n",
      "Epoch 94/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 20507556577280.0000 - val_loss: 21240328749056.0000\n",
      "Epoch 95/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 20327933411328.0000 - val_loss: 21105406377984.0000\n",
      "Epoch 96/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 20286919409664.0000 - val_loss: 20964616175616.0000\n",
      "Epoch 97/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 20128326483968.0000 - val_loss: 20823465263104.0000\n",
      "Epoch 98/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 20148918419456.0000 - val_loss: 20689742462976.0000\n",
      "Epoch 99/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 19879159660544.0000 - val_loss: 20548084039680.0000\n",
      "Epoch 100/1500\n",
      "48/48 [==============================] - 0s 776us/step - loss: 19836197404672.0000 - val_loss: 20408352899072.0000\n",
      "Epoch 101/1500\n",
      "48/48 [==============================] - 0s 711us/step - loss: 19507020038144.0000 - val_loss: 20263003488256.0000\n",
      "Epoch 102/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 19584281214976.0000 - val_loss: 20117243035648.0000\n",
      "Epoch 103/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 19447039393792.0000 - val_loss: 19976366850048.0000\n",
      "Epoch 104/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 19242476896256.0000 - val_loss: 19832011489280.0000\n",
      "Epoch 105/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 19100279504896.0000 - val_loss: 19688268496896.0000\n",
      "Epoch 106/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 18911409995776.0000 - val_loss: 19536594075648.0000\n",
      "Epoch 107/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 18810174177280.0000 - val_loss: 19389984276480.0000\n",
      "Epoch 108/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 18596411473920.0000 - val_loss: 19239503134720.0000\n",
      "Epoch 109/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 18636525797376.0000 - val_loss: 19087658844160.0000\n",
      "Epoch 110/1500\n",
      "48/48 [==============================] - 0s 795us/step - loss: 18478591377408.0000 - val_loss: 18944022806528.0000\n",
      "Epoch 111/1500\n",
      "48/48 [==============================] - 0s 751us/step - loss: 18352059711488.0000 - val_loss: 18795970166784.0000\n",
      "Epoch 112/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 18197365391360.0000 - val_loss: 18641525407744.0000\n",
      "Epoch 113/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 17987807477760.0000 - val_loss: 18493395173376.0000\n",
      "Epoch 114/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 18030614544384.0000 - val_loss: 18343920664576.0000\n",
      "Epoch 115/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 17939891748864.0000 - val_loss: 18190134411264.0000\n",
      "Epoch 116/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 17789247029248.0000 - val_loss: 18039351279616.0000\n",
      "Epoch 117/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 17356910755840.0000 - val_loss: 17887995625472.0000\n",
      "Epoch 118/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 17359101231104.0000 - val_loss: 17735364902912.0000\n",
      "Epoch 119/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 17362266882048.0000 - val_loss: 17583558361088.0000\n",
      "Epoch 120/1500\n",
      "48/48 [==============================] - 0s 999us/step - loss: 17098935894016.0000 - val_loss: 17431818928128.0000\n",
      "Epoch 121/1500\n",
      "48/48 [==============================] - 0s 746us/step - loss: 16935776419840.0000 - val_loss: 17275700641792.0000\n",
      "Epoch 122/1500\n",
      "48/48 [==============================] - 0s 800us/step - loss: 16864549797888.0000 - val_loss: 17126769295360.0000\n",
      "Epoch 123/1500\n",
      "48/48 [==============================] - 0s 762us/step - loss: 16678784073728.0000 - val_loss: 16969587752960.0000\n",
      "Epoch 124/1500\n",
      "48/48 [==============================] - 0s 755us/step - loss: 16485178146816.0000 - val_loss: 16814978367488.0000\n",
      "Epoch 125/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 16312956878848.0000 - val_loss: 16658821283840.0000\n",
      "Epoch 126/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 16438319382528.0000 - val_loss: 16509349920768.0000\n",
      "Epoch 127/1500\n",
      "48/48 [==============================] - 0s 707us/step - loss: 15963189673984.0000 - val_loss: 16354774089728.0000\n",
      "Epoch 128/1500\n",
      "48/48 [==============================] - 0s 699us/step - loss: 15892912013312.0000 - val_loss: 16199487324160.0000\n",
      "Epoch 129/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 15808160858112.0000 - val_loss: 16048416882688.0000\n",
      "Epoch 130/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 15746689138688.0000 - val_loss: 15892438056960.0000\n",
      "Epoch 131/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 15622848118784.0000 - val_loss: 15742440308736.0000\n",
      "Epoch 132/1500\n",
      "48/48 [==============================] - 0s 707us/step - loss: 15265653850112.0000 - val_loss: 15585283932160.0000\n",
      "Epoch 133/1500\n",
      "48/48 [==============================] - 0s 790us/step - loss: 15176869871616.0000 - val_loss: 15434410622976.0000\n",
      "Epoch 134/1500\n",
      "48/48 [==============================] - 0s 730us/step - loss: 15078204112896.0000 - val_loss: 15285688991744.0000\n",
      "Epoch 135/1500\n",
      "48/48 [==============================] - 0s 705us/step - loss: 15053436747776.0000 - val_loss: 15134361649152.0000\n",
      "Epoch 136/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 14788257120256.0000 - val_loss: 14982883311616.0000\n",
      "Epoch 137/1500\n",
      "48/48 [==============================] - 0s 707us/step - loss: 14899609600000.0000 - val_loss: 14834282266624.0000\n",
      "Epoch 138/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 14580053966848.0000 - val_loss: 14680743477248.0000\n",
      "Epoch 139/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 14146011660288.0000 - val_loss: 14527429083136.0000\n",
      "Epoch 140/1500\n",
      "48/48 [==============================] - 0s 700us/step - loss: 14426499448832.0000 - val_loss: 14385678385152.0000\n",
      "Epoch 141/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 14107948351488.0000 - val_loss: 14238700535808.0000\n",
      "Epoch 142/1500\n",
      "48/48 [==============================] - 0s 711us/step - loss: 13774854553600.0000 - val_loss: 14088654553088.0000\n",
      "Epoch 143/1500\n",
      "48/48 [==============================] - 0s 752us/step - loss: 14085530845184.0000 - val_loss: 13945143296000.0000\n",
      "Epoch 144/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 13815951392768.0000 - val_loss: 13797587681280.0000\n",
      "Epoch 145/1500\n",
      "48/48 [==============================] - 0s 702us/step - loss: 13750519201792.0000 - val_loss: 13651967737856.0000\n",
      "Epoch 146/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 13471710183424.0000 - val_loss: 13507951067136.0000\n",
      "Epoch 147/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 13443764584448.0000 - val_loss: 13368358338560.0000\n",
      "Epoch 148/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 13163405770752.0000 - val_loss: 13221064867840.0000\n",
      "Epoch 149/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 12987305820160.0000 - val_loss: 13077419393024.0000\n",
      "Epoch 150/1500\n",
      "48/48 [==============================] - 0s 701us/step - loss: 12963929915392.0000 - val_loss: 12934189154304.0000\n",
      "Epoch 151/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 12957068034048.0000 - val_loss: 12797389832192.0000\n",
      "Epoch 152/1500\n",
      "48/48 [==============================] - 0s 764us/step - loss: 12715284234240.0000 - val_loss: 12662254600192.0000\n",
      "Epoch 153/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 12575385321472.0000 - val_loss: 12527533555712.0000\n",
      "Epoch 154/1500\n",
      "48/48 [==============================] - 0s 693us/step - loss: 12340755955712.0000 - val_loss: 12385992572928.0000\n",
      "Epoch 155/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 11907026124800.0000 - val_loss: 12253050961920.0000\n",
      "Epoch 156/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 12228285693952.0000 - val_loss: 12117515173888.0000\n",
      "Epoch 157/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 12058725711872.0000 - val_loss: 11987219120128.0000\n",
      "Epoch 158/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 11868204695552.0000 - val_loss: 11851854249984.0000\n",
      "Epoch 159/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 11709911662592.0000 - val_loss: 11725993672704.0000\n",
      "Epoch 160/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 11782616776704.0000 - val_loss: 11595475320832.0000\n",
      "Epoch 161/1500\n",
      "48/48 [==============================] - 0s 997us/step - loss: 11648940113920.0000 - val_loss: 11473787027456.0000\n",
      "Epoch 162/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 11290316636160.0000 - val_loss: 11345174986752.0000\n",
      "Epoch 163/1500\n",
      "48/48 [==============================] - 0s 701us/step - loss: 11307769135104.0000 - val_loss: 11219892174848.0000\n",
      "Epoch 164/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 11154734710784.0000 - val_loss: 11099804008448.0000\n",
      "Epoch 165/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 11117525991424.0000 - val_loss: 10978280341504.0000\n",
      "Epoch 166/1500\n",
      "48/48 [==============================] - 0s 695us/step - loss: 11014871449600.0000 - val_loss: 10858330587136.0000\n",
      "Epoch 167/1500\n",
      "48/48 [==============================] - 0s 735us/step - loss: 10804897251328.0000 - val_loss: 10744538071040.0000\n",
      "Epoch 168/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 10824118697984.0000 - val_loss: 10630687883264.0000\n",
      "Epoch 169/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 10733384368128.0000 - val_loss: 10513870225408.0000\n",
      "Epoch 170/1500\n",
      "48/48 [==============================] - 0s 791us/step - loss: 10710204547072.0000 - val_loss: 10403521232896.0000\n",
      "Epoch 171/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 10396649914368.0000 - val_loss: 10289092231168.0000\n",
      "Epoch 172/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 10247571767296.0000 - val_loss: 10180623335424.0000\n",
      "Epoch 173/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 10051595010048.0000 - val_loss: 10074842988544.0000\n",
      "Epoch 174/1500\n",
      "48/48 [==============================] - 0s 693us/step - loss: 10227212615680.0000 - val_loss: 9965982973952.0000\n",
      "Epoch 175/1500\n",
      "48/48 [==============================] - 0s 756us/step - loss: 10113536491520.0000 - val_loss: 9858616131584.0000\n",
      "Epoch 176/1500\n",
      "48/48 [==============================] - 0s 745us/step - loss: 10008109514752.0000 - val_loss: 9758550523904.0000\n",
      "Epoch 177/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 9786586300416.0000 - val_loss: 9661042393088.0000\n",
      "Epoch 178/1500\n",
      "48/48 [==============================] - 0s 752us/step - loss: 9673233137664.0000 - val_loss: 9562646118400.0000\n",
      "Epoch 179/1500\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9653616377856.0000 - val_loss: 9467563343872.0000\n",
      "Epoch 180/1500\n",
      "48/48 [==============================] - 0s 971us/step - loss: 9556559134720.0000 - val_loss: 9373448404992.0000\n",
      "Epoch 181/1500\n",
      "48/48 [==============================] - 0s 748us/step - loss: 9533237755904.0000 - val_loss: 9275105607680.0000\n",
      "Epoch 182/1500\n",
      "48/48 [==============================] - 0s 745us/step - loss: 9244227141632.0000 - val_loss: 9186185314304.0000\n",
      "Epoch 183/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 9077502509056.0000 - val_loss: 9098410065920.0000\n",
      "Epoch 184/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 8994289614848.0000 - val_loss: 9010859212800.0000\n",
      "Epoch 185/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 9029755600896.0000 - val_loss: 8930123055104.0000\n",
      "Epoch 186/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 8960603062272.0000 - val_loss: 8843261116416.0000\n",
      "Epoch 187/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 8910589132800.0000 - val_loss: 8761314902016.0000\n",
      "Epoch 188/1500\n",
      "48/48 [==============================] - 0s 699us/step - loss: 9010052857856.0000 - val_loss: 8679157399552.0000\n",
      "Epoch 189/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 8755448119296.0000 - val_loss: 8600265162752.0000\n",
      "Epoch 190/1500\n",
      "48/48 [==============================] - 0s 810us/step - loss: 8877124878336.0000 - val_loss: 8521123364864.0000\n",
      "Epoch 191/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 8634245316608.0000 - val_loss: 8448106299392.0000\n",
      "Epoch 192/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 8431189098496.0000 - val_loss: 8372337246208.0000\n",
      "Epoch 193/1500\n",
      "48/48 [==============================] - 0s 711us/step - loss: 8481078771712.0000 - val_loss: 8299209031680.0000\n",
      "Epoch 194/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 8297515581440.0000 - val_loss: 8229830000640.0000\n",
      "Epoch 195/1500\n",
      "48/48 [==============================] - 0s 708us/step - loss: 8337620992000.0000 - val_loss: 8164970856448.0000\n",
      "Epoch 196/1500\n",
      "48/48 [==============================] - 0s 735us/step - loss: 8245643051008.0000 - val_loss: 8099346776064.0000\n",
      "Epoch 197/1500\n",
      "48/48 [==============================] - 0s 748us/step - loss: 8393617571840.0000 - val_loss: 8036174266368.0000\n",
      "Epoch 198/1500\n",
      "48/48 [==============================] - 0s 807us/step - loss: 7984855908352.0000 - val_loss: 7976168980480.0000\n",
      "Epoch 199/1500\n",
      "48/48 [==============================] - 0s 852us/step - loss: 7943551975424.0000 - val_loss: 7918717501440.0000\n",
      "Epoch 200/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 7815727939584.0000 - val_loss: 7863220043776.0000\n",
      "Epoch 201/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 7871697256448.0000 - val_loss: 7806123507712.0000\n",
      "Epoch 202/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 7791767453696.0000 - val_loss: 7748915822592.0000\n",
      "Epoch 203/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 7676956246016.0000 - val_loss: 7693681033216.0000\n",
      "Epoch 204/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 7812551278592.0000 - val_loss: 7639803101184.0000\n",
      "Epoch 205/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 7550870224896.0000 - val_loss: 7588516724736.0000\n",
      "Epoch 206/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 7480993120256.0000 - val_loss: 7537526046720.0000\n",
      "Epoch 207/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 7586184691712.0000 - val_loss: 7488002850816.0000\n",
      "Epoch 208/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 7367297073152.0000 - val_loss: 7439179055104.0000\n",
      "Epoch 209/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 7440378626048.0000 - val_loss: 7393075789824.0000\n",
      "Epoch 210/1500\n",
      "48/48 [==============================] - 0s 697us/step - loss: 7655514439680.0000 - val_loss: 7348574224384.0000\n",
      "Epoch 211/1500\n",
      "48/48 [==============================] - 0s 762us/step - loss: 7514522386432.0000 - val_loss: 7302474104832.0000\n",
      "Epoch 212/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 7409769119744.0000 - val_loss: 7260134703104.0000\n",
      "Epoch 213/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 7202604580864.0000 - val_loss: 7214867677184.0000\n",
      "Epoch 214/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 7326931615744.0000 - val_loss: 7174255280128.0000\n",
      "Epoch 215/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 7274844651520.0000 - val_loss: 7133581541376.0000\n",
      "Epoch 216/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 7019032477696.0000 - val_loss: 7091280936960.0000\n",
      "Epoch 217/1500\n",
      "48/48 [==============================] - 0s 943us/step - loss: 7325790240768.0000 - val_loss: 7052680232960.0000\n",
      "Epoch 218/1500\n",
      "48/48 [==============================] - 0s 770us/step - loss: 7156477722624.0000 - val_loss: 7017844965376.0000\n",
      "Epoch 219/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 7066866941952.0000 - val_loss: 6981094473728.0000\n",
      "Epoch 220/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 7237692030976.0000 - val_loss: 6943547064320.0000\n",
      "Epoch 221/1500\n",
      "48/48 [==============================] - 0s 703us/step - loss: 7203721838592.0000 - val_loss: 6909527064576.0000\n",
      "Epoch 222/1500\n",
      "48/48 [==============================] - 0s 746us/step - loss: 6937398738944.0000 - val_loss: 6876003565568.0000\n",
      "Epoch 223/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 7037637885952.0000 - val_loss: 6840177393664.0000\n",
      "Epoch 224/1500\n",
      "48/48 [==============================] - 0s 779us/step - loss: 7248390127616.0000 - val_loss: 6809841565696.0000\n",
      "Epoch 225/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 6856669396992.0000 - val_loss: 6773814067200.0000\n",
      "Epoch 226/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 6756167581696.0000 - val_loss: 6743714693120.0000\n",
      "Epoch 227/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 6530025914368.0000 - val_loss: 6711152738304.0000\n",
      "Epoch 228/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 6721912176640.0000 - val_loss: 6679391371264.0000\n",
      "Epoch 229/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 6474372743168.0000 - val_loss: 6649607618560.0000\n",
      "Epoch 230/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 6552480645120.0000 - val_loss: 6619453194240.0000\n",
      "Epoch 231/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 6914356281344.0000 - val_loss: 6591782846464.0000\n",
      "Epoch 232/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 6523494334464.0000 - val_loss: 6561374142464.0000\n",
      "Epoch 233/1500\n",
      "48/48 [==============================] - 0s 697us/step - loss: 6628696391680.0000 - val_loss: 6534396379136.0000\n",
      "Epoch 234/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 6751181078528.0000 - val_loss: 6505598287872.0000\n",
      "Epoch 235/1500\n",
      "48/48 [==============================] - 0s 705us/step - loss: 6577153638400.0000 - val_loss: 6477126303744.0000\n",
      "Epoch 236/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 6644165509120.0000 - val_loss: 6448768090112.0000\n",
      "Epoch 237/1500\n",
      "48/48 [==============================] - 0s 788us/step - loss: 6347451006976.0000 - val_loss: 6420258357248.0000\n",
      "Epoch 238/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 6183713243136.0000 - val_loss: 6395141816320.0000\n",
      "Epoch 239/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 6515404570624.0000 - val_loss: 6367397019648.0000\n",
      "Epoch 240/1500\n",
      "48/48 [==============================] - 0s 699us/step - loss: 6095371763712.0000 - val_loss: 6339225452544.0000\n",
      "Epoch 241/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 6234078969856.0000 - val_loss: 6311834025984.0000\n",
      "Epoch 242/1500\n",
      "48/48 [==============================] - 0s 759us/step - loss: 6307890331648.0000 - val_loss: 6285059686400.0000\n",
      "Epoch 243/1500\n",
      "48/48 [==============================] - 0s 757us/step - loss: 6289663459328.0000 - val_loss: 6258257559552.0000\n",
      "Epoch 244/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 6153807331328.0000 - val_loss: 6230547365888.0000\n",
      "Epoch 245/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 6705552818176.0000 - val_loss: 6205981327360.0000\n",
      "Epoch 246/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 5912677842944.0000 - val_loss: 6178587279360.0000\n",
      "Epoch 247/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 6409810870272.0000 - val_loss: 6153676783616.0000\n",
      "Epoch 248/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 6327050960896.0000 - val_loss: 6128746364928.0000\n",
      "Epoch 249/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 6300776267776.0000 - val_loss: 6102753214464.0000\n",
      "Epoch 250/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 6241585725440.0000 - val_loss: 6075723022336.0000\n",
      "Epoch 251/1500\n",
      "48/48 [==============================] - 0s 760us/step - loss: 6137769885696.0000 - val_loss: 6050184429568.0000\n",
      "Epoch 252/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 6076646817792.0000 - val_loss: 6024864989184.0000\n",
      "Epoch 253/1500\n",
      "48/48 [==============================] - 0s 703us/step - loss: 5987417718784.0000 - val_loss: 5997719977984.0000\n",
      "Epoch 254/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 6167665836032.0000 - val_loss: 5974351937536.0000\n",
      "Epoch 255/1500\n",
      "48/48 [==============================] - 0s 884us/step - loss: 5876451115008.0000 - val_loss: 5949052944384.0000\n",
      "Epoch 256/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 5939226214400.0000 - val_loss: 5923543187456.0000\n",
      "Epoch 257/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 5860511186944.0000 - val_loss: 5898023993344.0000\n",
      "Epoch 258/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 5714593972224.0000 - val_loss: 5871890857984.0000\n",
      "Epoch 259/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 5721297518592.0000 - val_loss: 5848584683520.0000\n",
      "Epoch 260/1500\n",
      "48/48 [==============================] - 0s 760us/step - loss: 5672950824960.0000 - val_loss: 5822894571520.0000\n",
      "Epoch 261/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 5822865211392.0000 - val_loss: 5796710055936.0000\n",
      "Epoch 262/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 5937787568128.0000 - val_loss: 5769859170304.0000\n",
      "Epoch 263/1500\n",
      "48/48 [==============================] - 0s 700us/step - loss: 6111298060288.0000 - val_loss: 5742200881152.0000\n",
      "Epoch 264/1500\n",
      "48/48 [==============================] - 0s 708us/step - loss: 5629543972864.0000 - val_loss: 5717516353536.0000\n",
      "Epoch 265/1500\n",
      "48/48 [==============================] - 0s 695us/step - loss: 5619290996736.0000 - val_loss: 5692293382144.0000\n",
      "Epoch 266/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 5856952844288.0000 - val_loss: 5665361756160.0000\n",
      "Epoch 267/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 5696780238848.0000 - val_loss: 5639803240448.0000\n",
      "Epoch 268/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 5375455657984.0000 - val_loss: 5614732312576.0000\n",
      "Epoch 269/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 5661713235968.0000 - val_loss: 5588791590912.0000\n",
      "Epoch 270/1500\n",
      "48/48 [==============================] - 0s 708us/step - loss: 5329718870016.0000 - val_loss: 5565720821760.0000\n",
      "Epoch 271/1500\n",
      "48/48 [==============================] - 0s 698us/step - loss: 5517692370944.0000 - val_loss: 5538279587840.0000\n",
      "Epoch 272/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 5316249911296.0000 - val_loss: 5516697796608.0000\n",
      "Epoch 273/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 5660109963264.0000 - val_loss: 5493305114624.0000\n",
      "Epoch 274/1500\n",
      "48/48 [==============================] - 0s 711us/step - loss: 5391871115264.0000 - val_loss: 5469100834816.0000\n",
      "Epoch 275/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 5704705900544.0000 - val_loss: 5443497754624.0000\n",
      "Epoch 276/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 5279653560320.0000 - val_loss: 5419302912000.0000\n",
      "Epoch 277/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 5433877594112.0000 - val_loss: 5391517745152.0000\n",
      "Epoch 278/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 5139014352896.0000 - val_loss: 5367054991360.0000\n",
      "Epoch 279/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 5495586291712.0000 - val_loss: 5340475686912.0000\n",
      "Epoch 280/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 5331196837888.0000 - val_loss: 5312529563648.0000\n",
      "Epoch 281/1500\n",
      "48/48 [==============================] - 0s 699us/step - loss: 5119538102272.0000 - val_loss: 5287709769728.0000\n",
      "Epoch 282/1500\n",
      "48/48 [==============================] - 0s 683us/step - loss: 5245595287552.0000 - val_loss: 5261568245760.0000\n",
      "Epoch 283/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 4996361355264.0000 - val_loss: 5238401531904.0000\n",
      "Epoch 284/1500\n",
      "48/48 [==============================] - 0s 707us/step - loss: 5287674118144.0000 - val_loss: 5211393884160.0000\n",
      "Epoch 285/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 5169618616320.0000 - val_loss: 5187358949376.0000\n",
      "Epoch 286/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 5260600934400.0000 - val_loss: 5160854618112.0000\n",
      "Epoch 287/1500\n",
      "48/48 [==============================] - 0s 694us/step - loss: 5261252100096.0000 - val_loss: 5134525399040.0000\n",
      "Epoch 288/1500\n",
      "48/48 [==============================] - 0s 689us/step - loss: 5363123879936.0000 - val_loss: 5108077690880.0000\n",
      "Epoch 289/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 5102235025408.0000 - val_loss: 5081612156928.0000\n",
      "Epoch 290/1500\n",
      "48/48 [==============================] - 0s 810us/step - loss: 4842207051776.0000 - val_loss: 5056583172096.0000\n",
      "Epoch 291/1500\n",
      "48/48 [==============================] - 0s 861us/step - loss: 5101048561664.0000 - val_loss: 5032301297664.0000\n",
      "Epoch 292/1500\n",
      "48/48 [==============================] - 0s 761us/step - loss: 5073340989440.0000 - val_loss: 5006025555968.0000\n",
      "Epoch 293/1500\n",
      "48/48 [==============================] - 0s 694us/step - loss: 5156065247232.0000 - val_loss: 4980643725312.0000\n",
      "Epoch 294/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 4754551341056.0000 - val_loss: 4956632383488.0000\n",
      "Epoch 295/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 4860713893888.0000 - val_loss: 4931827793920.0000\n",
      "Epoch 296/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 4800154435584.0000 - val_loss: 4905670017024.0000\n",
      "Epoch 297/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 4744029405184.0000 - val_loss: 4879746596864.0000\n",
      "Epoch 298/1500\n",
      "48/48 [==============================] - 0s 694us/step - loss: 4781654409216.0000 - val_loss: 4851200163840.0000\n",
      "Epoch 299/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 4998125060096.0000 - val_loss: 4825197576192.0000\n",
      "Epoch 300/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 4887142727680.0000 - val_loss: 4798855249920.0000\n",
      "Epoch 301/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 4554057318400.0000 - val_loss: 4772864196608.0000\n",
      "Epoch 302/1500\n",
      "48/48 [==============================] - 0s 754us/step - loss: 4736597098496.0000 - val_loss: 4746341515264.0000\n",
      "Epoch 303/1500\n",
      "48/48 [==============================] - 0s 711us/step - loss: 4666165821440.0000 - val_loss: 4721319870464.0000\n",
      "Epoch 304/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 4590568734720.0000 - val_loss: 4697624150016.0000\n",
      "Epoch 305/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 4707725082624.0000 - val_loss: 4670654251008.0000\n",
      "Epoch 306/1500\n",
      "48/48 [==============================] - 0s 703us/step - loss: 4608504627200.0000 - val_loss: 4643939155968.0000\n",
      "Epoch 307/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 4666114441216.0000 - val_loss: 4618800070656.0000\n",
      "Epoch 308/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 4432695656448.0000 - val_loss: 4595729825792.0000\n",
      "Epoch 309/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 4779449778176.0000 - val_loss: 4568479956992.0000\n",
      "Epoch 310/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 4729827491840.0000 - val_loss: 4542120329216.0000\n",
      "Epoch 311/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 4483149463552.0000 - val_loss: 4517837930496.0000\n",
      "Epoch 312/1500\n",
      "48/48 [==============================] - 0s 695us/step - loss: 4295746650112.0000 - val_loss: 4494181531648.0000\n",
      "Epoch 313/1500\n",
      "48/48 [==============================] - 0s 701us/step - loss: 4652232867840.0000 - val_loss: 4466868224000.0000\n",
      "Epoch 314/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 4556709691392.0000 - val_loss: 4439041638400.0000\n",
      "Epoch 315/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 4479854313472.0000 - val_loss: 4413397139456.0000\n",
      "Epoch 316/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 4365899005952.0000 - val_loss: 4386549923840.0000\n",
      "Epoch 317/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 4451630841856.0000 - val_loss: 4361342418944.0000\n",
      "Epoch 318/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 4284636725248.0000 - val_loss: 4334300954624.0000\n",
      "Epoch 319/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 4355202220032.0000 - val_loss: 4308406370304.0000\n",
      "Epoch 320/1500\n",
      "48/48 [==============================] - 0s 780us/step - loss: 4244292501504.0000 - val_loss: 4284921413632.0000\n",
      "Epoch 321/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 4357416288256.0000 - val_loss: 4257897775104.0000\n",
      "Epoch 322/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 4055618027520.0000 - val_loss: 4234040311808.0000\n",
      "Epoch 323/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 4447925174272.0000 - val_loss: 4209213964288.0000\n",
      "Epoch 324/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 4190020567040.0000 - val_loss: 4184022450176.0000\n",
      "Epoch 325/1500\n",
      "48/48 [==============================] - 0s 706us/step - loss: 4040291516416.0000 - val_loss: 4159022301184.0000\n",
      "Epoch 326/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 4350322409472.0000 - val_loss: 4131653419008.0000\n",
      "Epoch 327/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 4206040186880.0000 - val_loss: 4104477212672.0000\n",
      "Epoch 328/1500\n",
      "48/48 [==============================] - 0s 706us/step - loss: 3989046034432.0000 - val_loss: 4080407674880.0000\n",
      "Epoch 329/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 4253095559168.0000 - val_loss: 4056329486336.0000\n",
      "Epoch 330/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 4054319890432.0000 - val_loss: 4032176324608.0000\n",
      "Epoch 331/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 4221775904768.0000 - val_loss: 4007439368192.0000\n",
      "Epoch 332/1500\n",
      "48/48 [==============================] - 0s 767us/step - loss: 4165129469952.0000 - val_loss: 3982541193216.0000\n",
      "Epoch 333/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 4139890245632.0000 - val_loss: 3955835011072.0000\n",
      "Epoch 334/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 3882610589696.0000 - val_loss: 3931273953280.0000\n",
      "Epoch 335/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 4107414274048.0000 - val_loss: 3905695252480.0000\n",
      "Epoch 336/1500\n",
      "48/48 [==============================] - 0s 667us/step - loss: 4181260238848.0000 - val_loss: 3882477682688.0000\n",
      "Epoch 337/1500\n",
      "48/48 [==============================] - 0s 701us/step - loss: 3878919077888.0000 - val_loss: 3858282577920.0000\n",
      "Epoch 338/1500\n",
      "48/48 [==============================] - 0s 752us/step - loss: 3897540476928.0000 - val_loss: 3832635195392.0000\n",
      "Epoch 339/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 3731030540288.0000 - val_loss: 3808444809216.0000\n",
      "Epoch 340/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 3731905576960.0000 - val_loss: 3783196409856.0000\n",
      "Epoch 341/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 3815911456768.0000 - val_loss: 3757439188992.0000\n",
      "Epoch 342/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 3648176259072.0000 - val_loss: 3734600155136.0000\n",
      "Epoch 343/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 3786637312000.0000 - val_loss: 3712628031488.0000\n",
      "Epoch 344/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 3720203730944.0000 - val_loss: 3688794947584.0000\n",
      "Epoch 345/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 3819717001216.0000 - val_loss: 3662783709184.0000\n",
      "Epoch 346/1500\n",
      "48/48 [==============================] - 0s 698us/step - loss: 4018041520128.0000 - val_loss: 3639212507136.0000\n",
      "Epoch 347/1500\n",
      "48/48 [==============================] - 0s 691us/step - loss: 3519144787968.0000 - val_loss: 3616290111488.0000\n",
      "Epoch 348/1500\n",
      "48/48 [==============================] - 0s 685us/step - loss: 3852700745728.0000 - val_loss: 3595245191168.0000\n",
      "Epoch 349/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 3664942989312.0000 - val_loss: 3571995639808.0000\n",
      "Epoch 350/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 3796599570432.0000 - val_loss: 3546459144192.0000\n",
      "Epoch 351/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 3727070330880.0000 - val_loss: 3520181829632.0000\n",
      "Epoch 352/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 3585598816256.0000 - val_loss: 3496262500352.0000\n",
      "Epoch 353/1500\n",
      "48/48 [==============================] - 0s 711us/step - loss: 3402756784128.0000 - val_loss: 3473979998208.0000\n",
      "Epoch 354/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 3488587186176.0000 - val_loss: 3451363524608.0000\n",
      "Epoch 355/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 3505407131648.0000 - val_loss: 3429426266112.0000\n",
      "Epoch 356/1500\n",
      "48/48 [==============================] - 0s 787us/step - loss: 3429188239360.0000 - val_loss: 3408167698432.0000\n",
      "Epoch 357/1500\n",
      "48/48 [==============================] - 0s 706us/step - loss: 3479960551424.0000 - val_loss: 3386072629248.0000\n",
      "Epoch 358/1500\n",
      "48/48 [==============================] - 0s 979us/step - loss: 3427888005120.0000 - val_loss: 3364297375744.0000\n",
      "Epoch 359/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 3340507283456.0000 - val_loss: 3340643074048.0000\n",
      "Epoch 360/1500\n",
      "48/48 [==============================] - 0s 699us/step - loss: 3205217910784.0000 - val_loss: 3321469599744.0000\n",
      "Epoch 361/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 3597924302848.0000 - val_loss: 3299431940096.0000\n",
      "Epoch 362/1500\n",
      "48/48 [==============================] - 0s 768us/step - loss: 3243708252160.0000 - val_loss: 3279643213824.0000\n",
      "Epoch 363/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 3272150089728.0000 - val_loss: 3259087192064.0000\n",
      "Epoch 364/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 3307135827968.0000 - val_loss: 3238259064832.0000\n",
      "Epoch 365/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 3121342054400.0000 - val_loss: 3218812436480.0000\n",
      "Epoch 366/1500\n",
      "48/48 [==============================] - 0s 701us/step - loss: 3471484911616.0000 - val_loss: 3197258432512.0000\n",
      "Epoch 367/1500\n",
      "48/48 [==============================] - 0s 698us/step - loss: 3133619568640.0000 - val_loss: 3176284553216.0000\n",
      "Epoch 368/1500\n",
      "48/48 [==============================] - 0s 771us/step - loss: 3191500701696.0000 - val_loss: 3156717862912.0000\n",
      "Epoch 369/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 3069282091008.0000 - val_loss: 3137619886080.0000\n",
      "Epoch 370/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 3396796678144.0000 - val_loss: 3115681579008.0000\n",
      "Epoch 371/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 3412091207680.0000 - val_loss: 3096590680064.0000\n",
      "Epoch 372/1500\n",
      "48/48 [==============================] - 0s 952us/step - loss: 3147129421824.0000 - val_loss: 3075014393856.0000\n",
      "Epoch 373/1500\n",
      "48/48 [==============================] - 0s 706us/step - loss: 3189621915648.0000 - val_loss: 3054999699456.0000\n",
      "Epoch 374/1500\n",
      "48/48 [==============================] - 0s 759us/step - loss: 3309339148288.0000 - val_loss: 3036589326336.0000\n",
      "Epoch 375/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 3180558286848.0000 - val_loss: 3018208575488.0000\n",
      "Epoch 376/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 3107864969216.0000 - val_loss: 2997909454848.0000\n",
      "Epoch 377/1500\n",
      "48/48 [==============================] - 0s 707us/step - loss: 3157522907136.0000 - val_loss: 2978631122944.0000\n",
      "Epoch 378/1500\n",
      "48/48 [==============================] - 0s 706us/step - loss: 3158571745280.0000 - val_loss: 2959948644352.0000\n",
      "Epoch 379/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 2908855730176.0000 - val_loss: 2941114384384.0000\n",
      "Epoch 380/1500\n",
      "48/48 [==============================] - 0s 769us/step - loss: 2915369222144.0000 - val_loss: 2923419140096.0000\n",
      "Epoch 381/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 3171210493952.0000 - val_loss: 2901589098496.0000\n",
      "Epoch 382/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 3079746355200.0000 - val_loss: 2884905205760.0000\n",
      "Epoch 383/1500\n",
      "48/48 [==============================] - 0s 708us/step - loss: 2993817387008.0000 - val_loss: 2869627715584.0000\n",
      "Epoch 384/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 3207823360000.0000 - val_loss: 2850767765504.0000\n",
      "Epoch 385/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2686432051200.0000 - val_loss: 2832667508736.0000\n",
      "Epoch 386/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2693956108288.0000 - val_loss: 2813810180096.0000\n",
      "Epoch 387/1500\n",
      "48/48 [==============================] - 0s 764us/step - loss: 2784147537920.0000 - val_loss: 2794963861504.0000\n",
      "Epoch 388/1500\n",
      "48/48 [==============================] - 0s 761us/step - loss: 3133920509952.0000 - val_loss: 2776118853632.0000\n",
      "Epoch 389/1500\n",
      "48/48 [==============================] - 0s 745us/step - loss: 3262023467008.0000 - val_loss: 2759088406528.0000\n",
      "Epoch 390/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 2887611056128.0000 - val_loss: 2743664640000.0000\n",
      "Epoch 391/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 2745083101184.0000 - val_loss: 2727171063808.0000\n",
      "Epoch 392/1500\n",
      "48/48 [==============================] - 0s 756us/step - loss: 2686456430592.0000 - val_loss: 2712142086144.0000\n",
      "Epoch 393/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 2877637787648.0000 - val_loss: 2696134524928.0000\n",
      "Epoch 394/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 2945361903616.0000 - val_loss: 2679470817280.0000\n",
      "Epoch 395/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 2799979986944.0000 - val_loss: 2664396488704.0000\n",
      "Epoch 396/1500\n",
      "48/48 [==============================] - 0s 689us/step - loss: 2537536094208.0000 - val_loss: 2649063948288.0000\n",
      "Epoch 397/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 2583166451712.0000 - val_loss: 2635128897536.0000\n",
      "Epoch 398/1500\n",
      "48/48 [==============================] - 0s 746us/step - loss: 2718209933312.0000 - val_loss: 2620444901376.0000\n",
      "Epoch 399/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2788302782464.0000 - val_loss: 2606727954432.0000\n",
      "Epoch 400/1500\n",
      "48/48 [==============================] - 0s 745us/step - loss: 2583873978368.0000 - val_loss: 2592075153408.0000\n",
      "Epoch 401/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 2499084812288.0000 - val_loss: 2577835491328.0000\n",
      "Epoch 402/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 2548150566912.0000 - val_loss: 2564796186624.0000\n",
      "Epoch 403/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 2615793156096.0000 - val_loss: 2551841030144.0000\n",
      "Epoch 404/1500\n",
      "48/48 [==============================] - 0s 798us/step - loss: 2679796662272.0000 - val_loss: 2537956048896.0000\n",
      "Epoch 405/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 2638177107968.0000 - val_loss: 2523324743680.0000\n",
      "Epoch 406/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 2673126932480.0000 - val_loss: 2508924387328.0000\n",
      "Epoch 407/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 2602879942656.0000 - val_loss: 2495144263680.0000\n",
      "Epoch 408/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 2566233784320.0000 - val_loss: 2482531729408.0000\n",
      "Epoch 409/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 2633720922112.0000 - val_loss: 2472889024512.0000\n",
      "Epoch 410/1500\n",
      "48/48 [==============================] - 0s 775us/step - loss: 2437944442880.0000 - val_loss: 2460895150080.0000\n",
      "Epoch 411/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 2574511243264.0000 - val_loss: 2448640704512.0000\n",
      "Epoch 412/1500\n",
      "48/48 [==============================] - 0s 817us/step - loss: 2463006195712.0000 - val_loss: 2435452502016.0000\n",
      "Epoch 413/1500\n",
      "48/48 [==============================] - 0s 797us/step - loss: 2530480226304.0000 - val_loss: 2421127380992.0000\n",
      "Epoch 414/1500\n",
      "48/48 [==============================] - 0s 711us/step - loss: 2274203795456.0000 - val_loss: 2410656038912.0000\n",
      "Epoch 415/1500\n",
      "48/48 [==============================] - 0s 757us/step - loss: 2528199573504.0000 - val_loss: 2398260035584.0000\n",
      "Epoch 416/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 2611696893952.0000 - val_loss: 2385277878272.0000\n",
      "Epoch 417/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 2212090085376.0000 - val_loss: 2372983586816.0000\n",
      "Epoch 418/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 2436310237184.0000 - val_loss: 2362233847808.0000\n",
      "Epoch 419/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 2307203006464.0000 - val_loss: 2351297200128.0000\n",
      "Epoch 420/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 2400574504960.0000 - val_loss: 2339286548480.0000\n",
      "Epoch 421/1500\n",
      "48/48 [==============================] - 0s 805us/step - loss: 2502564249600.0000 - val_loss: 2329072107520.0000\n",
      "Epoch 422/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 2596446404608.0000 - val_loss: 2319356526592.0000\n",
      "Epoch 423/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 2376704196608.0000 - val_loss: 2309409472512.0000\n",
      "Epoch 424/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 2398713282560.0000 - val_loss: 2298264158208.0000\n",
      "Epoch 425/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2546625937408.0000 - val_loss: 2289730584576.0000\n",
      "Epoch 426/1500\n",
      "48/48 [==============================] - 0s 875us/step - loss: 2225521819648.0000 - val_loss: 2279527415808.0000\n",
      "Epoch 427/1500\n",
      "48/48 [==============================] - 0s 809us/step - loss: 2372984111104.0000 - val_loss: 2271928647680.0000\n",
      "Epoch 428/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 2292149911552.0000 - val_loss: 2261627437056.0000\n",
      "Epoch 429/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 2459240759296.0000 - val_loss: 2252166397952.0000\n",
      "Epoch 430/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 2212280926208.0000 - val_loss: 2242290122752.0000\n",
      "Epoch 431/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 2350842118144.0000 - val_loss: 2232227987456.0000\n",
      "Epoch 432/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 2475193270272.0000 - val_loss: 2225300570112.0000\n",
      "Epoch 433/1500\n",
      "48/48 [==============================] - 0s 748us/step - loss: 2243334242304.0000 - val_loss: 2217433890816.0000\n",
      "Epoch 434/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 2217699442688.0000 - val_loss: 2210965749760.0000\n",
      "Epoch 435/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 2122123182080.0000 - val_loss: 2203559133184.0000\n",
      "Epoch 436/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 2352110108672.0000 - val_loss: 2196738932736.0000\n",
      "Epoch 437/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 2455110942720.0000 - val_loss: 2190422310912.0000\n",
      "Epoch 438/1500\n",
      "48/48 [==============================] - 0s 825us/step - loss: 2189423149056.0000 - val_loss: 2185408282624.0000\n",
      "Epoch 439/1500\n",
      "48/48 [==============================] - 0s 920us/step - loss: 2161920311296.0000 - val_loss: 2178417295360.0000\n",
      "Epoch 440/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 2230140010496.0000 - val_loss: 2170424262656.0000\n",
      "Epoch 441/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 2053742395392.0000 - val_loss: 2164883062784.0000\n",
      "Epoch 442/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 2209824899072.0000 - val_loss: 2158219886592.0000\n",
      "Epoch 443/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 2048389283840.0000 - val_loss: 2150266568704.0000\n",
      "Epoch 444/1500\n",
      "48/48 [==============================] - 0s 698us/step - loss: 2322042716160.0000 - val_loss: 2144707411968.0000\n",
      "Epoch 445/1500\n",
      "48/48 [==============================] - 0s 782us/step - loss: 2061889961984.0000 - val_loss: 2139113914368.0000\n",
      "Epoch 446/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 2320558194688.0000 - val_loss: 2131407142912.0000\n",
      "Epoch 447/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 2264379424768.0000 - val_loss: 2125477707776.0000\n",
      "Epoch 448/1500\n",
      "48/48 [==============================] - 0s 714us/step - loss: 2235405434880.0000 - val_loss: 2119138803712.0000\n",
      "Epoch 449/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 2389689499648.0000 - val_loss: 2112560037888.0000\n",
      "Epoch 450/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 2122124230656.0000 - val_loss: 2106845560832.0000\n",
      "Epoch 451/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2071280222208.0000 - val_loss: 2101758918656.0000\n",
      "Epoch 452/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 2155419271168.0000 - val_loss: 2097082793984.0000\n",
      "Epoch 453/1500\n",
      "48/48 [==============================] - 0s 730us/step - loss: 2289968349184.0000 - val_loss: 2091313790976.0000\n",
      "Epoch 454/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 2345039822848.0000 - val_loss: 2083617898496.0000\n",
      "Epoch 455/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 2054188171264.0000 - val_loss: 2077345710080.0000\n",
      "Epoch 456/1500\n",
      "48/48 [==============================] - 0s 750us/step - loss: 2078688280576.0000 - val_loss: 2071880794112.0000\n",
      "Epoch 457/1500\n",
      "48/48 [==============================] - 0s 767us/step - loss: 2116309221376.0000 - val_loss: 2066230411264.0000\n",
      "Epoch 458/1500\n",
      "48/48 [==============================] - 0s 754us/step - loss: 2083352215552.0000 - val_loss: 2060935757824.0000\n",
      "Epoch 459/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 2079671320576.0000 - val_loss: 2055719616512.0000\n",
      "Epoch 460/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 2029566164992.0000 - val_loss: 2051817734144.0000\n",
      "Epoch 461/1500\n",
      "48/48 [==============================] - 0s 707us/step - loss: 2204963438592.0000 - val_loss: 2045859201024.0000\n",
      "Epoch 462/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 2331611758592.0000 - val_loss: 2041049382912.0000\n",
      "Epoch 463/1500\n",
      "48/48 [==============================] - 0s 711us/step - loss: 2362750271488.0000 - val_loss: 2035210125312.0000\n",
      "Epoch 464/1500\n",
      "48/48 [==============================] - 0s 766us/step - loss: 1963460526080.0000 - val_loss: 2032742825984.0000\n",
      "Epoch 465/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2108125478912.0000 - val_loss: 2026797137920.0000\n",
      "Epoch 466/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 2051526754304.0000 - val_loss: 2023671463936.0000\n",
      "Epoch 467/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 2286785921024.0000 - val_loss: 2019958587392.0000\n",
      "Epoch 468/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 1937018454016.0000 - val_loss: 2017204895744.0000\n",
      "Epoch 469/1500\n",
      "48/48 [==============================] - 0s 781us/step - loss: 2065116299264.0000 - val_loss: 2013369466880.0000\n",
      "Epoch 470/1500\n",
      "48/48 [==============================] - 0s 751us/step - loss: 1999296659456.0000 - val_loss: 2009828425728.0000\n",
      "Epoch 471/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 2266234617856.0000 - val_loss: 2006411247616.0000\n",
      "Epoch 472/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 2220271337472.0000 - val_loss: 2004673495040.0000\n",
      "Epoch 473/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 1943740481536.0000 - val_loss: 2001122230272.0000\n",
      "Epoch 474/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 2012184051712.0000 - val_loss: 1996532875264.0000\n",
      "Epoch 475/1500\n",
      "48/48 [==============================] - 0s 754us/step - loss: 2015092146176.0000 - val_loss: 1992061616128.0000\n",
      "Epoch 476/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 2013875666944.0000 - val_loss: 1988251877376.0000\n",
      "Epoch 477/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 2247765000192.0000 - val_loss: 1983653871616.0000\n",
      "Epoch 478/1500\n",
      "48/48 [==============================] - 0s 884us/step - loss: 2158076755968.0000 - val_loss: 1981110812672.0000\n",
      "Epoch 479/1500\n",
      "48/48 [==============================] - 0s 782us/step - loss: 2126739406848.0000 - val_loss: 1977265946624.0000\n",
      "Epoch 480/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 1921461911552.0000 - val_loss: 1975944216576.0000\n",
      "Epoch 481/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 2043779612672.0000 - val_loss: 1971543867392.0000\n",
      "Epoch 482/1500\n",
      "48/48 [==============================] - 0s 687us/step - loss: 2260452245504.0000 - val_loss: 1969459822592.0000\n",
      "Epoch 483/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 2017895514112.0000 - val_loss: 1967290187776.0000\n",
      "Epoch 484/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 2108896051200.0000 - val_loss: 1964048121856.0000\n",
      "Epoch 485/1500\n",
      "48/48 [==============================] - 0s 706us/step - loss: 1917182017536.0000 - val_loss: 1957990367232.0000\n",
      "Epoch 486/1500\n",
      "48/48 [==============================] - 0s 769us/step - loss: 2193851416576.0000 - val_loss: 1954846867456.0000\n",
      "Epoch 487/1500\n",
      "48/48 [==============================] - 0s 757us/step - loss: 2003377192960.0000 - val_loss: 1951044337664.0000\n",
      "Epoch 488/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 2123999084544.0000 - val_loss: 1948967501824.0000\n",
      "Epoch 489/1500\n",
      "48/48 [==============================] - 0s 730us/step - loss: 2139393753088.0000 - val_loss: 1945766854656.0000\n",
      "Epoch 490/1500\n",
      "48/48 [==============================] - 0s 961us/step - loss: 2040402542592.0000 - val_loss: 1942744989696.0000\n",
      "Epoch 491/1500\n",
      "48/48 [==============================] - 0s 752us/step - loss: 2043052949504.0000 - val_loss: 1940318978048.0000\n",
      "Epoch 492/1500\n",
      "48/48 [==============================] - 0s 765us/step - loss: 2182369509376.0000 - val_loss: 1938444910592.0000\n",
      "Epoch 493/1500\n",
      "48/48 [==============================] - 0s 730us/step - loss: 1814981640192.0000 - val_loss: 1935848243200.0000\n",
      "Epoch 494/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 2013421240320.0000 - val_loss: 1934199357440.0000\n",
      "Epoch 495/1500\n",
      "48/48 [==============================] - 0s 751us/step - loss: 2018533834752.0000 - val_loss: 1931316822016.0000\n",
      "Epoch 496/1500\n",
      "48/48 [==============================] - 0s 750us/step - loss: 2065928421376.0000 - val_loss: 1929888006144.0000\n",
      "Epoch 497/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 1878252584960.0000 - val_loss: 1927376535552.0000\n",
      "Epoch 498/1500\n",
      "48/48 [==============================] - 0s 761us/step - loss: 1840973348864.0000 - val_loss: 1926280118272.0000\n",
      "Epoch 499/1500\n",
      "48/48 [==============================] - 0s 708us/step - loss: 2323792265216.0000 - val_loss: 1924621271040.0000\n",
      "Epoch 500/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 2069741830144.0000 - val_loss: 1923397976064.0000\n",
      "Epoch 501/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 1965037715456.0000 - val_loss: 1922191458304.0000\n",
      "Epoch 502/1500\n",
      "48/48 [==============================] - 0s 772us/step - loss: 1927609843712.0000 - val_loss: 1918389977088.0000\n",
      "Epoch 503/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 2187663900672.0000 - val_loss: 1916173680640.0000\n",
      "Epoch 504/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1956949131264.0000 - val_loss: 1913313165312.0000\n",
      "Epoch 505/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 2040373182464.0000 - val_loss: 1911745937408.0000\n",
      "Epoch 506/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 2029680066560.0000 - val_loss: 1908766408704.0000\n",
      "Epoch 507/1500\n",
      "48/48 [==============================] - 0s 782us/step - loss: 1782431219712.0000 - val_loss: 1907097600000.0000\n",
      "Epoch 508/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 2025522200576.0000 - val_loss: 1905137418240.0000\n",
      "Epoch 509/1500\n",
      "48/48 [==============================] - 0s 702us/step - loss: 1725590536192.0000 - val_loss: 1901866778624.0000\n",
      "Epoch 510/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 1937075208192.0000 - val_loss: 1898706632704.0000\n",
      "Epoch 511/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 2029839187968.0000 - val_loss: 1896437776384.0000\n",
      "Epoch 512/1500\n",
      "48/48 [==============================] - 0s 774us/step - loss: 1939359006720.0000 - val_loss: 1893467422720.0000\n",
      "Epoch 513/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 2089689546752.0000 - val_loss: 1891951050752.0000\n",
      "Epoch 514/1500\n",
      "48/48 [==============================] - 0s 748us/step - loss: 1947600027648.0000 - val_loss: 1889991655424.0000\n",
      "Epoch 515/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 2069074542592.0000 - val_loss: 1886868996096.0000\n",
      "Epoch 516/1500\n",
      "48/48 [==============================] - 0s 973us/step - loss: 2164569931776.0000 - val_loss: 1884995321856.0000\n",
      "Epoch 517/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 2153432350720.0000 - val_loss: 1884893216768.0000\n",
      "Epoch 518/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 1977061605376.0000 - val_loss: 1882059309056.0000\n",
      "Epoch 519/1500\n",
      "48/48 [==============================] - 0s 750us/step - loss: 2074615349248.0000 - val_loss: 1881911197696.0000\n",
      "Epoch 520/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 1845889204224.0000 - val_loss: 1879926505472.0000\n",
      "Epoch 521/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 1990980665344.0000 - val_loss: 1877859237888.0000\n",
      "Epoch 522/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 1957058445312.0000 - val_loss: 1875135561728.0000\n",
      "Epoch 523/1500\n",
      "48/48 [==============================] - 0s 769us/step - loss: 2032475832320.0000 - val_loss: 1873253498880.0000\n",
      "Epoch 524/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 1814773104640.0000 - val_loss: 1872614785024.0000\n",
      "Epoch 525/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 2098739019776.0000 - val_loss: 1870248804352.0000\n",
      "Epoch 526/1500\n",
      "48/48 [==============================] - 0s 754us/step - loss: 1695205163008.0000 - val_loss: 1869062078464.0000\n",
      "Epoch 527/1500\n",
      "48/48 [==============================] - 0s 739us/step - loss: 2043155841024.0000 - val_loss: 1867894620160.0000\n",
      "Epoch 528/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1954200289280.0000 - val_loss: 1865142894592.0000\n",
      "Epoch 529/1500\n",
      "48/48 [==============================] - 0s 758us/step - loss: 1976766038016.0000 - val_loss: 1863474085888.0000\n",
      "Epoch 530/1500\n",
      "48/48 [==============================] - 0s 743us/step - loss: 1960326332416.0000 - val_loss: 1860778459136.0000\n",
      "Epoch 531/1500\n",
      "48/48 [==============================] - 0s 708us/step - loss: 1998895579136.0000 - val_loss: 1858438823936.0000\n",
      "Epoch 532/1500\n",
      "48/48 [==============================] - 0s 730us/step - loss: 1705062170624.0000 - val_loss: 1857760133120.0000\n",
      "Epoch 533/1500\n",
      "48/48 [==============================] - 0s 694us/step - loss: 2142335795200.0000 - val_loss: 1857580695552.0000\n",
      "Epoch 534/1500\n",
      "48/48 [==============================] - 0s 690us/step - loss: 2101406990336.0000 - val_loss: 1856728989696.0000\n",
      "Epoch 535/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 1891377872896.0000 - val_loss: 1855913197568.0000\n",
      "Epoch 536/1500\n",
      "48/48 [==============================] - 0s 702us/step - loss: 1976808505344.0000 - val_loss: 1855128600576.0000\n",
      "Epoch 537/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 2031633170432.0000 - val_loss: 1854291312640.0000\n",
      "Epoch 538/1500\n",
      "48/48 [==============================] - 0s 695us/step - loss: 1791741657088.0000 - val_loss: 1853402775552.0000\n",
      "Epoch 539/1500\n",
      "48/48 [==============================] - 0s 685us/step - loss: 2086486278144.0000 - val_loss: 1850379599872.0000\n",
      "Epoch 540/1500\n",
      "48/48 [==============================] - 0s 944us/step - loss: 1650995888128.0000 - val_loss: 1847761567744.0000\n",
      "Epoch 541/1500\n",
      "48/48 [==============================] - 0s 757us/step - loss: 1942775660544.0000 - val_loss: 1845703081984.0000\n",
      "Epoch 542/1500\n",
      "48/48 [==============================] - 0s 705us/step - loss: 1656395137024.0000 - val_loss: 1845483274240.0000\n",
      "Epoch 543/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 1845509226496.0000 - val_loss: 1844062191616.0000\n",
      "Epoch 544/1500\n",
      "48/48 [==============================] - 0s 670us/step - loss: 2165022785536.0000 - val_loss: 1843066961920.0000\n",
      "Epoch 545/1500\n",
      "48/48 [==============================] - 0s 697us/step - loss: 2189673103360.0000 - val_loss: 1839617277952.0000\n",
      "Epoch 546/1500\n",
      "48/48 [==============================] - 0s 772us/step - loss: 2121231892480.0000 - val_loss: 1838191738880.0000\n",
      "Epoch 547/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 1768156823552.0000 - val_loss: 1838572503040.0000\n",
      "Epoch 548/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 1740866584576.0000 - val_loss: 1837204766720.0000\n",
      "Epoch 549/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 2112684556288.0000 - val_loss: 1836207570944.0000\n",
      "Epoch 550/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 2250516987904.0000 - val_loss: 1835455217664.0000\n",
      "Epoch 551/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 2063262023680.0000 - val_loss: 1834693296128.0000\n",
      "Epoch 552/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 1877529722880.0000 - val_loss: 1834884792320.0000\n",
      "Epoch 553/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 1876163952640.0000 - val_loss: 1833359376384.0000\n",
      "Epoch 554/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 1740343869440.0000 - val_loss: 1832180383744.0000\n",
      "Epoch 555/1500\n",
      "48/48 [==============================] - 0s 726us/step - loss: 1905639292928.0000 - val_loss: 1830149947392.0000\n",
      "Epoch 556/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 1816924913664.0000 - val_loss: 1829033476096.0000\n",
      "Epoch 557/1500\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1995478401024.0000 - val_loss: 1828027236352.0000\n",
      "Epoch 558/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 1896444854272.0000 - val_loss: 1826947334144.0000\n",
      "Epoch 559/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 2136962498560.0000 - val_loss: 1825937293312.0000\n",
      "Epoch 560/1500\n",
      "48/48 [==============================] - 0s 735us/step - loss: 1887872352256.0000 - val_loss: 1824676511744.0000\n",
      "Epoch 561/1500\n",
      "48/48 [==============================] - 0s 699us/step - loss: 1915629076480.0000 - val_loss: 1823990087680.0000\n",
      "Epoch 562/1500\n",
      "48/48 [==============================] - 0s 787us/step - loss: 2133413330944.0000 - val_loss: 1823003115520.0000\n",
      "Epoch 563/1500\n",
      "48/48 [==============================] - 0s 720us/step - loss: 1881388220416.0000 - val_loss: 1821105455104.0000\n",
      "Epoch 564/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1791083937792.0000 - val_loss: 1820040888320.0000\n",
      "Epoch 565/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 1823320178688.0000 - val_loss: 1818064846848.0000\n",
      "Epoch 566/1500\n",
      "48/48 [==============================] - 0s 721us/step - loss: 1934519697408.0000 - val_loss: 1816422383616.0000\n",
      "Epoch 567/1500\n",
      "48/48 [==============================] - 0s 763us/step - loss: 1954666381312.0000 - val_loss: 1815617994752.0000\n",
      "Epoch 568/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 1855196889088.0000 - val_loss: 1814672572416.0000\n",
      "Epoch 569/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 1876157399040.0000 - val_loss: 1812771373056.0000\n",
      "Epoch 570/1500\n",
      "48/48 [==============================] - 0s 711us/step - loss: 1878112862208.0000 - val_loss: 1812496908288.0000\n",
      "Epoch 571/1500\n",
      "48/48 [==============================] - 0s 684us/step - loss: 1629997367296.0000 - val_loss: 1810397003776.0000\n",
      "Epoch 572/1500\n",
      "48/48 [==============================] - 0s 759us/step - loss: 1571770859520.0000 - val_loss: 1808046227456.0000\n",
      "Epoch 573/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 1840017309696.0000 - val_loss: 1806347665408.0000\n",
      "Epoch 574/1500\n",
      "48/48 [==============================] - 0s 717us/step - loss: 1952328974336.0000 - val_loss: 1804937854976.0000\n",
      "Epoch 575/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 1939513671680.0000 - val_loss: 1804038832128.0000\n",
      "Epoch 576/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1677629980672.0000 - val_loss: 1805335658496.0000\n",
      "Epoch 577/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 1914114801664.0000 - val_loss: 1804423135232.0000\n",
      "Epoch 578/1500\n",
      "48/48 [==============================] - 0s 776us/step - loss: 1608782315520.0000 - val_loss: 1802664411136.0000\n",
      "Epoch 579/1500\n",
      "48/48 [==============================] - 0s 730us/step - loss: 1904498966528.0000 - val_loss: 1801161408512.0000\n",
      "Epoch 580/1500\n",
      "48/48 [==============================] - 0s 729us/step - loss: 1813779841024.0000 - val_loss: 1799759724544.0000\n",
      "Epoch 581/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 1771647533056.0000 - val_loss: 1798778257408.0000\n",
      "Epoch 582/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 1935541534720.0000 - val_loss: 1796958060544.0000\n",
      "Epoch 583/1500\n",
      "48/48 [==============================] - 0s 753us/step - loss: 1836645089280.0000 - val_loss: 1796519624704.0000\n",
      "Epoch 584/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 1695764185088.0000 - val_loss: 1795781033984.0000\n",
      "Epoch 585/1500\n",
      "48/48 [==============================] - 0s 715us/step - loss: 1853665050624.0000 - val_loss: 1796458807296.0000\n",
      "Epoch 586/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 1942198812672.0000 - val_loss: 1796338352128.0000\n",
      "Epoch 587/1500\n",
      "48/48 [==============================] - 0s 971us/step - loss: 1783842603008.0000 - val_loss: 1794522742784.0000\n",
      "Epoch 588/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 1966569422848.0000 - val_loss: 1794051276800.0000\n",
      "Epoch 589/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 2080957661184.0000 - val_loss: 1792694026240.0000\n",
      "Epoch 590/1500\n",
      "48/48 [==============================] - 0s 727us/step - loss: 2019393798144.0000 - val_loss: 1790373265408.0000\n",
      "Epoch 591/1500\n",
      "48/48 [==============================] - 0s 704us/step - loss: 1926553665536.0000 - val_loss: 1788908535808.0000\n",
      "Epoch 592/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 1769483141120.0000 - val_loss: 1787922350080.0000\n",
      "Epoch 593/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 2022642810880.0000 - val_loss: 1785507479552.0000\n",
      "Epoch 594/1500\n",
      "48/48 [==============================] - 0s 742us/step - loss: 1975529242624.0000 - val_loss: 1785071665152.0000\n",
      "Epoch 595/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 2002421940224.0000 - val_loss: 1784435179520.0000\n",
      "Epoch 596/1500\n",
      "48/48 [==============================] - 0s 735us/step - loss: 1840138027008.0000 - val_loss: 1783519117312.0000\n",
      "Epoch 597/1500\n",
      "48/48 [==============================] - 0s 718us/step - loss: 1767064862720.0000 - val_loss: 1784156913664.0000\n",
      "Epoch 598/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 1786325106688.0000 - val_loss: 1782647357440.0000\n",
      "Epoch 599/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1821675225088.0000 - val_loss: 1782563864576.0000\n",
      "Epoch 600/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 1939944636416.0000 - val_loss: 1780377845760.0000\n",
      "Epoch 601/1500\n",
      "48/48 [==============================] - 0s 705us/step - loss: 1965081231360.0000 - val_loss: 1779879510016.0000\n",
      "Epoch 602/1500\n",
      "48/48 [==============================] - 0s 708us/step - loss: 1785461080064.0000 - val_loss: 1777439080448.0000\n",
      "Epoch 603/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 2068643446784.0000 - val_loss: 1775208890368.0000\n",
      "Epoch 604/1500\n",
      "48/48 [==============================] - 0s 773us/step - loss: 1640169340928.0000 - val_loss: 1774736113664.0000\n",
      "Epoch 605/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 1839092334592.0000 - val_loss: 1773541392384.0000\n",
      "Epoch 606/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 1874022629376.0000 - val_loss: 1771541626880.0000\n",
      "Epoch 607/1500\n",
      "48/48 [==============================] - 0s 704us/step - loss: 2169053380608.0000 - val_loss: 1772554158080.0000\n",
      "Epoch 608/1500\n",
      "48/48 [==============================] - 0s 708us/step - loss: 1615424258048.0000 - val_loss: 1770646536192.0000\n",
      "Epoch 609/1500\n",
      "48/48 [==============================] - 0s 764us/step - loss: 2061954842624.0000 - val_loss: 1770056056832.0000\n",
      "Epoch 610/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1941683699712.0000 - val_loss: 1769979248640.0000\n",
      "Epoch 611/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 1897839329280.0000 - val_loss: 1769519579136.0000\n",
      "Epoch 612/1500\n",
      "48/48 [==============================] - 0s 719us/step - loss: 1733597593600.0000 - val_loss: 1768932769792.0000\n",
      "Epoch 613/1500\n",
      "48/48 [==============================] - 0s 712us/step - loss: 1974601121792.0000 - val_loss: 1768201125888.0000\n",
      "Epoch 614/1500\n",
      "48/48 [==============================] - 0s 708us/step - loss: 1953400619008.0000 - val_loss: 1767544586240.0000\n",
      "Epoch 615/1500\n",
      "48/48 [==============================] - 0s 770us/step - loss: 2043839905792.0000 - val_loss: 1767224377344.0000\n",
      "Epoch 616/1500\n",
      "48/48 [==============================] - 0s 731us/step - loss: 2116843732992.0000 - val_loss: 1765087117312.0000\n",
      "Epoch 617/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 2072599199744.0000 - val_loss: 1765379932160.0000\n",
      "Epoch 618/1500\n",
      "48/48 [==============================] - 0s 724us/step - loss: 2099239452672.0000 - val_loss: 1764303044608.0000\n",
      "Epoch 619/1500\n",
      "48/48 [==============================] - 0s 705us/step - loss: 2090690936832.0000 - val_loss: 1763320004608.0000\n",
      "Epoch 620/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 1820310896640.0000 - val_loss: 1762847621120.0000\n",
      "Epoch 621/1500\n",
      "48/48 [==============================] - 0s 797us/step - loss: 1738326540288.0000 - val_loss: 1761792884736.0000\n",
      "Epoch 622/1500\n",
      "48/48 [==============================] - 0s 929us/step - loss: 1765557534720.0000 - val_loss: 1759192940544.0000\n",
      "Epoch 623/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 1996093915136.0000 - val_loss: 1760389758976.0000\n",
      "Epoch 624/1500\n",
      "48/48 [==============================] - 0s 708us/step - loss: 1897940647936.0000 - val_loss: 1760139673600.0000\n",
      "Epoch 625/1500\n",
      "48/48 [==============================] - 0s 756us/step - loss: 1966755807232.0000 - val_loss: 1759298584576.0000\n",
      "Epoch 626/1500\n",
      "48/48 [==============================] - 0s 738us/step - loss: 1932540510208.0000 - val_loss: 1757608542208.0000\n",
      "Epoch 627/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 1680253779968.0000 - val_loss: 1756273967104.0000\n",
      "Epoch 628/1500\n",
      "48/48 [==============================] - 0s 777us/step - loss: 1681195794432.0000 - val_loss: 1755905654784.0000\n",
      "Epoch 629/1500\n",
      "48/48 [==============================] - 0s 770us/step - loss: 1755606679552.0000 - val_loss: 1755839201280.0000\n",
      "Epoch 630/1500\n",
      "48/48 [==============================] - 0s 749us/step - loss: 1640056487936.0000 - val_loss: 1756744384512.0000\n",
      "Epoch 631/1500\n",
      "48/48 [==============================] - 0s 836us/step - loss: 1799207780352.0000 - val_loss: 1757321887744.0000\n",
      "Epoch 632/1500\n",
      "48/48 [==============================] - 0s 863us/step - loss: 1794548826112.0000 - val_loss: 1757767794688.0000\n",
      "Epoch 633/1500\n",
      "48/48 [==============================] - 0s 783us/step - loss: 1739888001024.0000 - val_loss: 1756691693568.0000\n",
      "Epoch 634/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 1968595140608.0000 - val_loss: 1755796865024.0000\n",
      "Epoch 635/1500\n",
      "48/48 [==============================] - 0s 694us/step - loss: 1915921760256.0000 - val_loss: 1754514194432.0000\n",
      "Epoch 636/1500\n",
      "48/48 [==============================] - 0s 770us/step - loss: 1783377559552.0000 - val_loss: 1754039058432.0000\n",
      "Epoch 637/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 1711247720448.0000 - val_loss: 1752540774400.0000\n",
      "Epoch 638/1500\n",
      "48/48 [==============================] - 0s 780us/step - loss: 1791476367360.0000 - val_loss: 1752117542912.0000\n",
      "Epoch 639/1500\n",
      "48/48 [==============================] - 0s 810us/step - loss: 1644300468224.0000 - val_loss: 1749338423296.0000\n",
      "Epoch 640/1500\n",
      "48/48 [==============================] - 0s 703us/step - loss: 1990351912960.0000 - val_loss: 1748957659136.0000\n",
      "Epoch 641/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 1711505145856.0000 - val_loss: 1748963950592.0000\n",
      "Epoch 642/1500\n",
      "48/48 [==============================] - 0s 728us/step - loss: 1877421850624.0000 - val_loss: 1748366786560.0000\n",
      "Epoch 643/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1869693059072.0000 - val_loss: 1748504281088.0000\n",
      "Epoch 644/1500\n",
      "48/48 [==============================] - 0s 751us/step - loss: 1726587994112.0000 - val_loss: 1750719528960.0000\n",
      "Epoch 645/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 1802864689152.0000 - val_loss: 1749193326592.0000\n",
      "Epoch 646/1500\n",
      "48/48 [==============================] - 0s 710us/step - loss: 1871656386560.0000 - val_loss: 1749844754432.0000\n",
      "Epoch 647/1500\n",
      "48/48 [==============================] - 0s 734us/step - loss: 1876277723136.0000 - val_loss: 1749255716864.0000\n",
      "Epoch 648/1500\n",
      "48/48 [==============================] - 0s 774us/step - loss: 1793237057536.0000 - val_loss: 1747404980224.0000\n",
      "Epoch 649/1500\n",
      "48/48 [==============================] - 0s 735us/step - loss: 1782271574016.0000 - val_loss: 1745730928640.0000\n",
      "Epoch 650/1500\n",
      "48/48 [==============================] - 0s 747us/step - loss: 1713670848512.0000 - val_loss: 1745755832320.0000\n",
      "Epoch 651/1500\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1747752583168.0000 - val_loss: 1743462465536.0000\n",
      "Epoch 652/1500\n",
      "48/48 [==============================] - 0s 777us/step - loss: 1770746937344.0000 - val_loss: 1741311967232.0000\n",
      "Epoch 653/1500\n",
      "48/48 [==============================] - 0s 824us/step - loss: 1646019608576.0000 - val_loss: 1739814469632.0000\n",
      "Epoch 654/1500\n",
      "48/48 [==============================] - 0s 948us/step - loss: 1649103601664.0000 - val_loss: 1739287691264.0000\n",
      "Epoch 655/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 1715364954112.0000 - val_loss: 1738784505856.0000\n",
      "Epoch 656/1500\n",
      "48/48 [==============================] - 0s 713us/step - loss: 1898626547712.0000 - val_loss: 1737538142208.0000\n",
      "Epoch 657/1500\n",
      "48/48 [==============================] - 0s 716us/step - loss: 1840587210752.0000 - val_loss: 1738170302464.0000\n",
      "Epoch 658/1500\n",
      "48/48 [==============================] - 0s 757us/step - loss: 1794067922944.0000 - val_loss: 1737693593600.0000\n",
      "Epoch 659/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 1851404976128.0000 - val_loss: 1736753676288.0000\n",
      "Epoch 660/1500\n",
      "48/48 [==============================] - 0s 722us/step - loss: 1896424669184.0000 - val_loss: 1736846344192.0000\n",
      "Epoch 661/1500\n",
      "48/48 [==============================] - 0s 725us/step - loss: 1911418388480.0000 - val_loss: 1736245641216.0000\n",
      "Epoch 662/1500\n",
      "48/48 [==============================] - 0s 801us/step - loss: 1932089884672.0000 - val_loss: 1736309473280.0000\n",
      "Epoch 663/1500\n",
      "48/48 [==============================] - 0s 758us/step - loss: 1590493052928.0000 - val_loss: 1735663157248.0000\n",
      "Epoch 664/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1682312003584.0000 - val_loss: 1736353120256.0000\n",
      "Epoch 665/1500\n",
      "48/48 [==============================] - 0s 746us/step - loss: 1763883089920.0000 - val_loss: 1735977861120.0000\n",
      "Epoch 666/1500\n",
      "48/48 [==============================] - 0s 755us/step - loss: 1846550200320.0000 - val_loss: 1734163824640.0000\n",
      "Epoch 667/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 1704899641344.0000 - val_loss: 1735065075712.0000\n",
      "Epoch 668/1500\n",
      "48/48 [==============================] - 0s 818us/step - loss: 1856331055104.0000 - val_loss: 1733574787072.0000\n",
      "Epoch 669/1500\n",
      "48/48 [==============================] - 0s 745us/step - loss: 1585596071936.0000 - val_loss: 1732683366400.0000\n",
      "Epoch 670/1500\n",
      "48/48 [==============================] - 0s 740us/step - loss: 2244407459840.0000 - val_loss: 1732157767680.0000\n",
      "Epoch 671/1500\n",
      "48/48 [==============================] - 0s 737us/step - loss: 1703503593472.0000 - val_loss: 1732921262080.0000\n",
      "Epoch 672/1500\n",
      "48/48 [==============================] - 0s 976us/step - loss: 1793810366464.0000 - val_loss: 1732154621952.0000\n",
      "Epoch 673/1500\n",
      "48/48 [==============================] - 0s 750us/step - loss: 1752676564992.0000 - val_loss: 1730959245312.0000\n",
      "Epoch 674/1500\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1659011203072.0000 - val_loss: 1728882540544.0000\n",
      "Epoch 675/1500\n",
      "48/48 [==============================] - 0s 741us/step - loss: 1841509826560.0000 - val_loss: 1727963987968.0000\n",
      "Epoch 676/1500\n",
      "48/48 [==============================] - 0s 736us/step - loss: 1570346500096.0000 - val_loss: 1727960711168.0000\n",
      "Epoch 677/1500\n",
      "48/48 [==============================] - 0s 754us/step - loss: 1984565477376.0000 - val_loss: 1726644748288.0000\n",
      "Epoch 678/1500\n",
      "48/48 [==============================] - 0s 893us/step - loss: 1888254033920.0000 - val_loss: 1725184344064.0000\n",
      "Epoch 679/1500\n",
      "48/48 [==============================] - 0s 797us/step - loss: 1941182742528.0000 - val_loss: 1725008183296.0000\n",
      "Epoch 680/1500\n",
      "48/48 [==============================] - 0s 782us/step - loss: 2017749762048.0000 - val_loss: 1724864921600.0000\n",
      "Epoch 681/1500\n",
      "48/48 [==============================] - 0s 801us/step - loss: 1805315473408.0000 - val_loss: 1725118808064.0000\n",
      "Epoch 682/1500\n",
      "48/48 [==============================] - 0s 775us/step - loss: 1871187148800.0000 - val_loss: 1725257351168.0000\n",
      "Epoch 683/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1706716692480.0000 - val_loss: 1724609593344.0000\n",
      "Epoch 684/1500\n",
      "48/48 [==============================] - 0s 820us/step - loss: 1820369879040.0000 - val_loss: 1723950563328.0000\n",
      "Epoch 685/1500\n",
      "48/48 [==============================] - 0s 723us/step - loss: 1947752857600.0000 - val_loss: 1722218708992.0000\n",
      "Epoch 686/1500\n",
      "48/48 [==============================] - 0s 702us/step - loss: 1895403749376.0000 - val_loss: 1720990433280.0000\n",
      "Epoch 687/1500\n",
      "48/48 [==============================] - 0s 709us/step - loss: 1716604239872.0000 - val_loss: 1722330775552.0000\n",
      "Epoch 688/1500\n",
      "48/48 [==============================] - 0s 706us/step - loss: 1958124322816.0000 - val_loss: 1723203977216.0000\n",
      "Epoch 689/1500\n",
      "48/48 [==============================] - 0s 778us/step - loss: 1729345617920.0000 - val_loss: 1722102448128.0000\n",
      "Epoch 690/1500\n",
      "48/48 [==============================] - 0s 732us/step - loss: 1871231451136.0000 - val_loss: 1723298742272.0000\n",
      "Epoch 691/1500\n",
      "48/48 [==============================] - 0s 733us/step - loss: 1752460034048.0000 - val_loss: 1721066979328.0000\n",
      "Epoch 692/1500\n",
      "48/48 [==============================] - 0s 767us/step - loss: 1782669508608.0000 - val_loss: 1721995231232.0000\n",
      "Epoch 693/1500\n",
      "48/48 [==============================] - 0s 860us/step - loss: 1860200693760.0000 - val_loss: 1722702102528.0000\n",
      "Epoch 694/1500\n",
      "48/48 [==============================] - 0s 1ms/step - loss: 1618459361280.0000 - val_loss: 1722452934656.0000\n",
      "Epoch 695/1500\n",
      "48/48 [==============================] - 0s 814us/step - loss: 1488331341824.0000 - val_loss: 1723044200448.0000\n",
      "Epoch 696/1500\n",
      "48/48 [==============================] - 0s 781us/step - loss: 1446498795520.0000 - val_loss: 1722497892352.0000\n"
     ]
    }
   ],
   "source": [
    "# Training (early stopping for val loss stall)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(X_train, y_train, epochs=1500, batch_size=8, validation_data=(X_val, y_val), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting losses\n",
    "epochs = range(1, len(history.history['loss']) + 1)\n",
    "\n",
    "plt.plot(epochs, history.history['loss'], 'b', label='Training Loss')\n",
    "plt.plot(epochs, history.history['val_loss'], 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 1354767532032.0000\n",
      "Test Loss: 1354767532032.0\n"
     ]
    }
   ],
   "source": [
    "# Testing model\n",
    "test_loss = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 790us/step\n",
      "Mean Squared Error: 1354767404387.2349\n"
     ]
    }
   ],
   "source": [
    "# Calculating mse\n",
    "predictions = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 875us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting prdictions vs. truth, housing prices are difficult lol\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "errors = np.abs(predictions.flatten() - y_test)\n",
    "threshold = 500000\n",
    "outlier_indices = np.where(errors > threshold)[0]\n",
    "\n",
    "plt.scatter(y_test, predictions, c='b', label='Non-Outliers', alpha=0.7)  # Non-outliers are in blue\n",
    "plt.scatter(y_test.iloc[outlier_indices], predictions[outlier_indices], c='r', label='Outliers', alpha=0.7)  # Outliers are in red\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'k--', lw=2)  # Plot the line of best fit (y = x)\n",
    "\n",
    "plt.xlabel('Actual Prices')\n",
    "plt.ylabel('Predicted Prices')\n",
    "plt.title('Actual vs. Predicted House Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 837us/step\n",
      "R-squared (R2) Score: 0.6844255038922091\n"
     ]
    }
   ],
   "source": [
    "# R2 score\n",
    "r2 = r2_score(y_test, model.predict(X_test))\n",
    "print(f\"R-squared (R2) Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohansrivastava/miniconda3/envs/ml_env/lib/python3.10/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Saving model\n",
    "model.save(\"models/r2_6844_model.h5\")\n",
    "\n",
    "# Saving model history\n",
    "with open(\"model_histories/r2_6844_model_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
